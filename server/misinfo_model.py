import os
import re
import html
import time
import json
import requests
from functools import lru_cache
from typing import List, Dict, Any
from urllib.parse import urlparse
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer, util
import google.generativeai as genai
from google.oauth2 import service_account
from google.auth.transport.requests import Request
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib

from database import db
from vectorDb import (
    embed_text,
    store_feedback,
    pc, INDEX_NAME
)
from datetime import datetime, timedelta
import aiohttp
import asyncio

#----------------- Gemini config ----------------
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("‚ùå Missing GEMINI_API_KEY in environment variables")
genai.configure(api_key=GEMINI_API_KEY)
GEM_MODEL = genai.GenerativeModel("gemini-2.5-flash")

# ---------------- Vertex AI config ----------------
PROJECT_ID = os.getenv("PROJECT_ID")
ENDPOINT_ID = os.getenv("TEXT_ENDPOINT_ID")
REGION = os.getenv("LOCATION", "us-central1")
PREDICT_URL = f"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}:predict"
SERVICE_ACCOUNT_PATH = os.getenv("SERVICE_ACCOUNT_PATH")
GOOGLE_KEY = os.getenv("GOOGLE_API_KEY")
CX_ID = os.getenv("GOOGLE_SEARCH_CX")
FACT_CHECK_API_KEY = os.getenv("GEMINI_API_KEY")

# ---------------- GCP credentials (for Vertex AI) ----------------
def get_gcp_credentials():
    """
    Load and refresh service account credentials for GCP (usable by Vertex AI).
    """
    creds = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_PATH,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    creds.refresh(Request())
    return creds

def get_access_token():
    creds = get_gcp_credentials()
    return creds.token

# ---------------- Embeddings ----------------
embedder = SentenceTransformer("all-MiniLM-L6-v2")

@lru_cache(maxsize=8192)  
def get_embedding(text: str):
    return embedder.encode(text, convert_to_tensor=True)

# ---------------- Constants ----------------

CLAIM_MIN_LEN = 30
MAX_SEARCH_RESULTS = 5
EMB_SIM_THRESHOLD = 0.40

# ---------------- Utilities ----------------
def retry(func, tries=3, delay=1.0):
    def wrapper(*args, **kwargs):
        for i in range(tries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                if i == tries - 1:
                    raise e
                time.sleep(delay * (2 ** i))
    return wrapper

def simple_sentence_split(text: str) -> List[str]:
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    sents = [s.strip() for s in sents if len(s.strip()) >= CLAIM_MIN_LEN]
    return sents[:3] or [text[:500]]

def clamp01(x: float) -> float:
    return max(0.0, min(1.0, x))

def domain_from_url(url: str) -> str:
    m = re.search(r"https?://(www\.)?([^/]+)", url)
    return m.group(2).lower() if m else ""

def get_trusted_score(domain: str) -> float:
    """Return avg_score of domain or 0 if not found"""
    domain = domain.lower().strip()
    doc = db.collection("news_sources").document(domain).get()
    if doc.exists:
        return doc.to_dict().get("avg_score", 0.0)
    return 0.0

def domain_score_for_url(url: str) -> float:
    """
    Return the score for a domain.
    Pulls from Firestore dynamically.
    """
    d = domain_from_url(url)
    score = get_trusted_score(d)
    return score

_CACHE_TTL = 300 
_last_cache_time = 0
_cached_domains = []

def invalidate_domain_cache():
    """Force cache invalidation (e.g. after updates)."""
    global _last_cache_time
    _last_cache_time = 0

def load_credible_domains_cached() -> List[str]:
    """Loads credible domains from Firestore with 5-min TTL cache."""
    global _cached_domains, _last_cache_time
    now = time.time()
    if not _cached_domains or (now - _last_cache_time) > _CACHE_TTL:
        _cached_domains = load_credible_domains()
        _last_cache_time = now
    return _cached_domains

def add_or_update_trusted_sources_batch(domain_scores: Dict[str, float]):
    """
    Batch update Firestore for multiple domains at once.
    domain_scores: {domain: new_score, ...}
    """
    batch = db.batch()
    for domain, new_score in domain_scores.items():
        domain = domain.lower().strip()
        doc_ref = db.collection("news_sources").document(domain)
        doc = doc_ref.get()
        if doc.exists:
            data = doc.to_dict()
            current_avg = data.get("avg_score", 0.0)
            num_votes = data.get("num_votes", 0)
            updated_avg = round((current_avg * num_votes + new_score) / (num_votes + 1), 3)
            batch.update(doc_ref, {
                "avg_score": updated_avg,
                "num_votes": num_votes + 1,
                "last_updated": datetime.utcnow()
            })
        else:
            batch.set(doc_ref, {
                "avg_score": round(new_score, 3),
                "num_votes": 1,
                "last_updated": datetime.utcnow()
            })
    batch.commit()
    invalidate_domain_cache()

# ---------------- Gemini helper ----------------
@retry
def ask_gemini_structured(prompt: str) -> Dict[str, Any]:
    try:
        resp = GEM_MODEL.generate_content(prompt)
        text = ""
        try:
            text = resp.candidates[0].content.parts[0].text.strip()
        except Exception:
            text = getattr(resp, "text", "").strip() or str(resp)
        try:
            parsed = json.loads(text)
            return {"parsed": parsed, "raw_text": text}
        except Exception:
            match = re.search(r"\{[\s\S]*\}", text)
            if match:
                try:
                    parsed = json.loads(match.group(0))
                    return {"parsed": parsed, "raw_text": text}
                except Exception:
                    pass
            return {"parsed": {}, "raw_text": text}

    except Exception as e:
        return {"error": str(e), "parsed": {}}

# ---------------- Google Fact Check API ----------------
def query_google_fact_check_api(text: str, max_results=5) -> dict:
    import requests, re

    try:
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        candidates = [s for s in sentences if 5 <= len(s.split()) <= 20]
        refined = max(candidates, key=len, default=text[:100]).strip()

        resp = requests.get(
            "https://factchecktools.googleapis.com/v1alpha1/claims:search",
            params={
                "key": FACT_CHECK_API_KEY,
                "query": refined,
                "pageSize": max_results,
                "languageCode": "en"
            },
            timeout=6
        )

        if resp.status_code != 200:
            return {
                "status": "api_error",
                "fact_checks": [],
                "summary": {"total": 0, "false_count": 0, "true_count": 0, "mixed_count": 0}
            }

        claims = resp.json().get("claims", [])
        if not claims:
            return {
                "status": "no_fact_checks",
                "fact_checks": [],
                "summary": {"total": 0, "false_count": 0, "true_count": 0, "mixed_count": 0}
            }

        fact_checks = []
        counters = {"false": 0, "true": 0, "mixed": 0}

        for claim in claims[:max_results]:
            text_snippet = claim.get("text", "")[:150]
            reviews = claim.get("claimReview", [])[:2]

            for r in reviews:
                rating = r.get("textualRating", "").lower()

                if "false" in rating or "fake" in rating or "incorrect" in rating or "misleading" in rating or "pants" in rating:
                    cat = "false"
                elif "true" in rating or "correct" in rating or "accurate" in rating or "verified" in rating:
                    cat = "true"
                elif "mixed" in rating or "partial" in rating or "mostly" in rating or "half" in rating:
                    cat = "mixed"
                else:
                    cat = "unknown"

                if cat in counters:
                    counters[cat] += 1

                fact_checks.append({
                    "claim": text_snippet,
                    "publisher": r.get("publisher", {}).get("name", "Unknown"),
                    "rating": rating,
                    "rating_category": cat,
                    "title": r.get("title", ""),
                    "url": r.get("url", "")
                })

        total = len(fact_checks)
        if not total:
            return {
                "status": "no_fact_checks",
                "fact_checks": [],
                "summary": {"total": 0, "false_count": 0, "true_count": 0, "mixed_count": 0}
            }

        fr = counters["false"] / total
        tr = counters["true"] / total

        if fr >= 0.6:
            status = "predominantly_false"
        elif tr >= 0.6:
            status = "predominantly_true"
        elif counters["mixed"] >= 2:
            status = "mixed_ratings"
        else:
            status = "inconclusive"

        return {
            "status": status,
            "fact_checks": fact_checks,
            "summary": {
                "total": total,
                "false_count": counters["false"],
                "true_count": counters["true"],
                "mixed_count": counters["mixed"]
            }
        }

    except Exception as e:
        return {
            "status": "error",
            "fact_checks": [],
            "summary": {"total": 0, "false_count": 0, "true_count": 0, "mixed_count": 0},
            "error": str(e)
        }

def extract_metadata_with_gemini(text: str) -> dict:
    try:
        prompt = f""" Extract structured information from the following news article text. Return only valid JSON with keys: title, text, author, date, source, category. Rules: - Infer 'title' and 'category' from the text. - If 'author' or 'source' is not present, use "Unknown". - If 'date' is missing, use today's date in YYYY-MM-DD. Text: {text} """
        gem_resp = ask_gemini_structured(prompt)
        parsed = gem_resp.get("parsed", {})
        return {
            "title": parsed.get("title") or "Inferred",
            "text": parsed.get("text") or text[:4000],
            "author": parsed.get("author") or "Unknown",
            "date": parsed.get("date") or datetime.now().strftime("%Y-%m-%d"),
            "source": parsed.get("source") or "Unknown",
            "category": parsed.get("category") or "Inferred"
        }
    except Exception:
        return {
            "title": "Inferred",
            "text": text[:4000],
            "author": "Unknown",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "source": "Unknown",
            "category": "Inferred"
        }

@retry
def predict_with_vertex_ai(metadata: dict) -> dict:

    try:
        headers = {
            "Authorization": f"Bearer {get_access_token()}",
            "Content-Type": "application/json"
        }

        response = requests.post(
            PREDICT_URL,
            headers=headers,
            json={"instances": [metadata]},
            timeout=15
        )

        if response.status_code != 200:
            print(f"[Vertex AI] ‚ö†Ô∏è Endpoint returned {response.status_code}: {response.text[:200]}")
            return {"predictions": [{"classes": ["Real", "Fake", "Misleading"], "scores": [0.7, 0.2, 0.1]}]}

        try:
            data = response.json()
        except Exception as e:
            print(f"[Vertex AI] ‚ö†Ô∏è Invalid JSON response: {e}")
            return {"predictions": [{"classes": ["Real", "Fake", "Misleading"], "scores": [0.7, 0.2, 0.1]}]}

        print("[Vertex AI] ‚úÖ Response received successfully")
        return data

    except requests.exceptions.Timeout:
        print("[Vertex AI] ‚è∞ Timeout ‚Äî using fallback prediction.")
    except requests.exceptions.ConnectionError:
        print("[Vertex AI] üåê Connection error ‚Äî spot instance may be unavailable.")
    except Exception as e:
        print(f"[Vertex AI] ‚ö†Ô∏è Unexpected error: {e}")

    return {"predictions": [{"classes": ["Real", "Fake", "Misleading"], "scores": [0.7, 0.2, 0.1]}]}


def extract_vertex_scores(vertex_result: dict) -> dict:
    try:
        preds = vertex_result.get("predictions", [{}])[0]
        classes = preds.get("classes", [])
        scores = preds.get("scores", [])

        if not isinstance(classes, list) or not isinstance(scores, list):
            raise ValueError("Invalid Vertex response shape")

        score_map = {cls.capitalize(): float(score) for cls, score in zip(classes, scores)}

        return {
            "Real": score_map.get("Real", 0.7),
            "Fake": score_map.get("Fake", 0.2),
            "Misleading": score_map.get("Misleading", 0.1)
        }

    except Exception as e:
        print(f"[Vertex AI] ‚ùå Score extraction failed ‚Äî Using fallback: {e}")
        return {"Real": 0.7, "Fake": 0.2, "Misleading": 0.1}


def clear_cache_for_text(text: str) -> bool:
    """
    Deletes cached Pinecone entries semantically matching the input text.
    Used to force a re-analysis if misinformation data has been updated.
    Returns True if a cache entry was found and deleted, else False.
    """
    try:
        query_emb = embed_text(text)
        if not query_emb:
            print("[Cache Clear] Could not generate embedding.")
            return False

        index = pc.Index(INDEX_NAME)
        search = index.query(vector=query_emb, top_k=1, include_metadata=True)

        if not search.matches:
            print("[Cache Clear] No similar cache entry found.")
            return False

        match_id = search.matches[0].id
        index.delete(ids=[match_id])
        print(f"[Cache Clear] Deleted Pinecone entry: {match_id}")
        return True

    except Exception as e:
        print(f"[Cache Clear Error] {e}")
        return False


def adjusted_ensemble(
    gem_pred: str,
    gem_conf: int,
    vertex_scores: dict,
    fact_check_status: str,
    corroboration_status: str,
    evidence_strength: float = 0.0,
    threshold=0.15
) -> tuple[str, int]:
    """
    Evidence-first ensemble logic.
    Prioritizes real-world corroboration > model inference > ML scoring.
    """

    # Normalize vertex scores
    C_real = vertex_scores.get("Real", 0.7)
    C_fake = vertex_scores.get("Fake", 0.3)
    C_mis = vertex_scores.get("Misleading", 0.0)

    # -------------------------------
    # 1Ô∏è‚É£ Fact-check overrides
    # -------------------------------
    if fact_check_status == "predominantly_false":
        return "Fake", min(97, gem_conf + 10)

    if fact_check_status == "predominantly_true":
        return "Real", min(98, max(gem_conf, 85) + 10)

    if fact_check_status == "mixed_ratings":
        return "Misleading", max(70, gem_conf)

    # -------------------------------
    # 2Ô∏è‚É£ Dynamic weight blending
    # -------------------------------
    # Evidence dominates; Gemini adjusts; Vertex adds minor modulation
    w_evidence = 0.6
    w_gemini = 0.25
    w_vertex = 0.15

    # Convert Gemini‚Äôs text prediction to probability space
    P_gem = {
        "Real": 0.7 if gem_pred == "Real" else 0.15,
        "Fake": 0.7 if gem_pred == "Fake" else 0.15,
        "Misleading": 0.6 if gem_pred == "Misleading" else 0.1,
    }

    # Map evidence_strength (-ve ‚Üí Fake, +ve ‚Üí Real)
    P_evidence_real = clamp01(0.5 + 0.5 * evidence_strength)  # [-1, +1] ‚Üí [0, 1]
    P_evidence_fake = 1.0 - P_evidence_real

    # Weighted fusion
    real_score = (
        w_evidence * P_evidence_real +
        w_gemini * P_gem["Real"] +
        w_vertex * C_real
    )

    fake_score = (
        w_evidence * P_evidence_fake +
        w_gemini * P_gem["Fake"] +
        w_vertex * C_fake
    )

    mis_score = (
        w_gemini * P_gem["Misleading"] +
        w_vertex * C_mis * 0.8
    )

    # Normalize and choose final label
    total = real_score + fake_score + mis_score
    probs = {
        "Real": real_score / total,
        "Fake": fake_score / total,
        "Misleading": mis_score / total
    }

    final_pred = max(probs, key=probs.get)
    final_conf = int(min(100, (max(probs.values()) * 100)))

    # -------------------------------
    # 3Ô∏è‚É£ Adjust based on corroboration status
    # -------------------------------
    if corroboration_status == "corroborated" and evidence_strength > 0.4:
        final_conf = min(100, final_conf + 10)

    elif corroboration_status == "weak":
        final_conf = max(60, final_conf - 5)

    elif corroboration_status == "no_results":
        final_conf = max(55, final_conf - 10)

    return final_pred, final_conf


def load_credible_domains() -> List[str]:
    """
    Return the list of currently trusted domains from Firestore.
    Only include domains with at least 1 vote.
    """
    docs = db.collection("news_sources").where("num_votes", ">=", 1).stream()
    domains = [doc.id for doc in docs]
    if not domains:
        domains = ["reuters.com", "bbc.com", "apnews.com", "cnn.com", "nytimes.com",
                   "theguardian.com", "npr.org", "aljazeera.com", "bloomberg.com"]
    return domains

def get_domain_bonus(domain: str) -> float:
    """
    Return a very small bonus (2%) if the domain is highly credible (avg_score >= 0.9)
    and has more than 100 votes. Returns 0 otherwise.
    """
    domain = domain.lower().strip().rstrip("/") 
    if not domain or domain in ["unknown", ""]:
        return 0.0
    try:
        doc_ref = db.collection("news_sources").document(domain)
        doc = doc_ref.get()
        if doc.exists:
            data = doc.to_dict()
            avg_score = data.get("avg_score", 0.0)
            num_votes = data.get("num_votes", 0)
            if avg_score >= 0.9 and num_votes > 100:
                return 0.02 
        return 0.0
    except Exception as e:
        print(f"‚ö†Ô∏è Domain bonus fetch failed for {domain}: {e}")
        return 0.0

async def fetch_google(session, query, num_results=None):
    try:
        limit = int(num_results or MAX_SEARCH_RESULTS)

        params = {
            "key": str(GOOGLE_KEY or ""),
            "cx": str(CX_ID or ""),
            "q": str(query or ""),
            "num": limit
        }

        async with session.get(
            "https://www.googleapis.com/customsearch/v1",
            params=params,
            timeout=10
        ) as resp:
            data = await resp.json()
            return data.get("items", [])[:limit]
    except Exception as e:
        print(f"‚ùå Google fetch failed: {e}")
        return []

async def extract_keywords(claim: str) -> List[str]:
    """
    Reformulate the claim into 3‚Äì6 compressed variations preserving meaning.
    Returns an array of alternate phrasing, not keywords.
    """
    try:
        prompt = f"""
Rewrite this claim into 3‚Äì6 alternative formulations that preserve meaning.
Each reformulation should be concise, factual, and under 25 words.
Return ONLY a JSON array of strings.

Text: {claim}
"""
        resp = await asyncio.to_thread(ask_gemini_structured, prompt)

        if "error" in resp:
            print(f"‚ö†Ô∏è Reformulation error: {resp['error']}")
            return []

        parsed = resp.get("parsed")

        # Accept JSON list
        if isinstance(parsed, list):
            variations = parsed
        # Sometimes model returns { "rewrites": [...] }
        elif isinstance(parsed, dict):
            variations = parsed.get("rewrites") or parsed.get("summaries") or parsed.get("sentences", [])
        # As fallback parse raw text into JSON-like list
        else:
            raw = resp.get("raw_text", "")
            variations = re.findall(r'"([^"]+)"', raw)

        # Cleanup
        clean = [
            v.strip()
            for v in variations
            if isinstance(v, str) and len(v.strip()) > 5
        ]

        return clean[:6]

    except Exception as e:
        print(f"‚ùå Reformulation exception: {e}")
        return []

def store_in_firestore(text, overall_conf, overall_label, combined_explanation):
    """
    Stores evaluated article into Firestore with the SAME schema as your original system.
    """
    try:
        embedding = [float(x) for x in get_embedding(text).tolist()]

        if db:
            doc_id = hashlib.sha256(text.encode("utf-8")).hexdigest()

            db.collection("articles").document(doc_id).set({
                "text": text,
                "embedding": embedding,
                "verified": True,
                "prediction": overall_label,
                "text_score": overall_conf / 100,
                "gemini_reasoning": combined_explanation,
                "text_explanation": combined_explanation,
                "last_updated": datetime.utcnow(),
                "type": "text"
            }, merge=True)

            print("[Firestore] ‚úÖ Stored successfully")
            return True

    except Exception as e:
        print(f"[Firestore Storage Error] ‚ùå {e}")

    return False

def store_in_pinecone(text, overall_conf, overall_label, combined_explanation):
    """
    Stores text + metadata into Pinecone using store_feedback()
    """
    try:
        store_feedback(
            text=text,
            explanation=combined_explanation,
            sources=[],                 
            user_fingerprint="system",
            score=overall_conf / 100,    
            prediction=overall_label,
            verified=True,
        )

        print("[Pinecone] ‚úÖ Stored successfully")
        return True

    except Exception as e:
        print(f"[Pinecone Store Error] ‚ùå {e}")

    return False

async def summarize_claim(claim: str) -> str:
    """Summarize claim into Google query with fallback."""
    try:
        prompt = f"""
Rewrite this as a concise Google search query (5-10 words).
Return ONLY the query text, no JSON, no quotes, no explanation.

Text: {claim}
"""
        resp = await asyncio.to_thread(ask_gemini_structured, prompt)
        
        if "error" in resp:
            print(f"‚ö†Ô∏è Summarization error: {resp['error']}")
            return claim[:100]
        
        # Check parsed first
        parsed = resp.get("parsed")
        if isinstance(parsed, dict):
            result = parsed.get("query", parsed.get("summary", ""))
            if result:
                return str(result).strip()[:150]
        elif isinstance(parsed, str):
            return parsed.strip()[:150]
        
        # Fallback to raw_text
        raw = resp.get("raw_text", "").strip()
        if raw:
            # Clean JSON artifacts
            clean = re.sub(r'[\{\}":]', '', raw).strip()
            # Remove common prefixes
            clean = re.sub(r'^(query|summary):\s*', '', clean, flags=re.IGNORECASE)
            if len(clean) >= 10:
                return clean[:150]
        
        # Last resort: use original claim
        return claim[:100]
        
    except Exception as e:
        print(f"‚ùå Summarization exception: {e}")
        return claim[:100]


async def corroborate_all_with_google_async(claims: List[str]) -> Dict[str, Any]:

    evidences = []
    CREDIBLE_DOMAINS = load_credible_domains_cached()
    domain_updates: Dict[str, float] = {}

    keyword_tasks = [extract_keywords(claim) for claim in claims]
    summary_tasks = [summarize_claim(claim) for claim in claims]

    keywords_list = await asyncio.gather(*keyword_tasks)
    summaries_list = await asyncio.gather(*summary_tasks)

    async with aiohttp.ClientSession() as session:
        tasks = []

        for claim, kws, summary in zip(claims, keywords_list, summaries_list):

            # ----------- fallback if summary is garbage -----------
            if not summary or len(summary) < 10 or "{" in summary or "error" in summary.lower():
                print(f"‚ö†Ô∏è Invalid summary, using original claim text instead")
                summary = claim[:120]

            # ----------- build google query -----------
            if kws:
                kw_str = " ".join(kws[:4])
                query = f"{summary} {kw_str}" if kw_str.lower() not in summary.lower() else summary
            else:
                query = summary

            print(f"üîç Google query => {query[:80]}")
            tasks.append(fetch_google(session, query, num_results=10))

        results_list = await asyncio.gather(*tasks)

    emb_cache = {}

    def get_emb(text):
        if text not in emb_cache:
            emb_cache[text] = get_embedding(text)
        return emb_cache[text]

    for claim, items in zip(claims, results_list):

        if not items:
            print(f"‚ö†Ô∏è No results returned from Google for claim: {claim[:60]}")
            continue

        articles = []
        for it in items[:8]: 
            snippet = html.unescape(it.get("snippet", "")).strip()
            if not snippet or len(snippet) < 20:
                continue

            articles.append({
                "title": it.get("title", "No title"),
                "snippet": snippet[:350],
                "link": it.get("link", "")
            })

        # ----------- Construct prompt for Gemini -----------
        gem_prompt = f"""
You evaluate whether news articles support a claim.

CLAIM: "{claim}"

ARTICLES:
{json.dumps(articles, indent=2)}
Also take into account the date of posting of the article dismiss the articles if another newer claim says the opposite of the older articles
Return STRICT JSON ONLY:

{{
 "evaluated": [
   {{
     "title": "...",
     "link": "...",
     "relevance": "supports" | "contradicts"| "unrelated",
     "confidence": 0-100
   }}
 ]
}}
"""

        gem_resp = ask_gemini_structured(gem_prompt)
        evaluated = gem_resp.get("parsed", {}).get("evaluated", []) if isinstance(gem_resp, dict) else []

        claim_evidences = []

        for result in evaluated:
            relevance = result.get("relevance")
            if relevance not in ["supports", "contradicts"]:
                continue

            link = result.get("link", "")
            domain = urlparse(link).netloc.lower()
            snippet = next((a["snippet"] for a in articles if a["link"] == link), "")

            # Compute similarity with embedding
            claim_emb = get_emb(claim)
            snippet_emb = get_emb(snippet)
            similarity = float(util.cos_sim(claim_emb, snippet_emb))

            # Domain score (credibility)
            domain_score = domain_score_for_url(link)

            # Weighted confidence (similarity + domain trust)
            evidence_score = round(clamp01(0.75 * similarity + 0.25 * domain_score), 3)

            claim_evidences.append({
                "title": result.get("title"),
                "link": link,
                "snippet": snippet,
                "similarity": round(similarity, 3),
                "domain_score": domain_score,
                "evidence_score": evidence_score,
                "is_new_domain": domain not in CREDIBLE_DOMAINS,
                "relevance": relevance,
                "confidence": result.get("confidence", 50)
            })

            # update trusted domains if high reliability
            if evidence_score > 0.7:
                domain_updates[domain] = evidence_score

        # Keep top 3 strongest pieces of evidence
        top = sorted(claim_evidences, key=lambda x: x["evidence_score"], reverse=True)[:3]
        evidences.extend(top)

    # Update trusted domain score DB
    if domain_updates:
        await asyncio.to_thread(add_or_update_trusted_sources_batch, domain_updates)

    # Final status classification
    status = (
        "corroborated" if len(evidences) >= 2 else
        "weak" if evidences else
        "no_results"
    )

    return {"status": status, "evidences": evidences}

def extract_local_context(claim: str, full_text: str, window: int = 2) -> str:
    sentences = re.split(r'(?<=[.!?])\s+', full_text.strip())
    best_idx = 0
    for i, sent in enumerate(sentences):
        if claim.strip()[:min(30, len(claim))].lower() in sent.lower():
            best_idx = i
            break
    start = max(0, best_idx - window)
    end = min(len(sentences), best_idx + window + 1)
    return " ".join(sentences[start:end])[:1200]

def assemble_gemini_prompt_structured(claim: str, evidences: List[Dict[str, Any]], status: str, fact_check_results: dict, full_text: str = "") -> str:
    today_str = datetime.now().strftime("%B %d, %Y")
    local_context = extract_local_context(claim, full_text) if full_text else ""

    context_part = f"The claim appears in the following context:\n\"\"\"{local_context}\"\"\"\n\n" if local_context else ""

    fact_checks_str = ""
    if fact_check_results["fact_checks"]:
        fact_checks_str = "\n".join([
            f"- {fc['publisher']}: \"{fc['rating']}\" ({fc['rating_category'].upper()}) - {fc['claim'][:100]}"
            for fc in fact_check_results["fact_checks"][:3]
        ])
    else:
        fact_checks_str = "No professional fact-checks found for this specific claim."
    
    fc_summary = fact_check_results["summary"]

    return f"""
You are an AI fact-checking assistant synthesizing ML predictions, search evidence, and professional fact-checks.

{context_part}
Input claim: \"\"\"{claim}\"\"\"
Corroboration status: {status}
Evidence snippets: {json.dumps(evidences[:5], ensure_ascii=False)}
Fact-Check Status: {fact_check_results['status']}
Fact-Check Summary:
{fact_checks_str}
   - Total fact-checks: {fc_summary['total']}
   - Rated FALSE: {fc_summary['false_count']} | TRUE: {fc_summary['true_count']} | MIXED: {fc_summary['mixed_count']}
Today's date: {today_str}

FIRST, ASSESS THE CONTENT TYPE:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Determine if this text contains VERIFIABLE FACTUAL CLAIMS that require fact-checking.

NON-FACTUAL CONTENT (Skip full analysis):
- Personal experiences: "My trip to Japan", "I visited the museum"
- Opinions/feelings: "This movie is amazing", "I think cats are better"
- Questions: "What happened in 2020?", "How do I cook pasta?"
- Creative content: Video titles, song lyrics, fiction, poems
- Instructions/how-tos: "Steps to make coffee"
- Promotional content: "Buy now!", "Best deals today"
- Greetings/casual chat: "Hello everyone", "Thanks for watching"

FACTUAL CONTENT (Requires analysis):
- News events: "Earthquake hits California", "President announces policy"
- Scientific claims: "Study shows coffee prevents cancer"
- Historical statements: "Napoleon died in 1821"
- Statistics: "Unemployment rate dropped to 3%"
- Allegations: "Company accused of fraud"

IF NON-FACTUAL CONTENT DETECTED:
Return JSON with:
- prediction: "Not Applicable"
- confidence: 100
- explanation: "This content does not contain verifiable factual claims requiring fact-checking | [Brief description of content type]"
- evidence: []
- content_type: "personal/opinion/question/creative/promotional/casual"

Example for non-factual:
{{
  "prediction": "Not Applicable",
  "confidence": 100,
  "explanation": "Personal travel content without factual claims | This appears to be a YouTube video title about someone's vacation",
  "evidence": [],
  "content_type": "personal"
}}

IF FACTUAL CONTENT DETECTED:
Proceed with full fact-checking analysis:

Instructions:
- Prioritize fact-check consensus if available (e.g., predominantly_false ‚Üí Fake).
- Use evidence snippets and ML context to verify factual accuracy.
- Evaluate the claim considering today's date ({today_str}).
- Consider temporal context: old news may be accurate but outdated.
- Return a strict JSON object with keys:

- prediction: "Real", "Fake", or "Misleading"
- confidence: integer 0‚Äì100
- explanation: 1‚Äì2 short, plain sentences | Use "|" to separate reasoning steps
- evidence: 1‚Äì3 key snippets (‚â§50 words each) with a `support` field
- content_type: "news"
- human_summary (optional): plain summary of the claim

Example for factual content:
{{
  "prediction": "Real",
  "confidence": 85,
  "explanation": "The claim matches current verified reports | Context indicates it refers to an ongoing event",
  "evidence": [
    {{"source":"BBC", "link":"https://...", "snippet":"BBC confirms the described protests occurred in Delhi.", "support":"Supports"}}
  ],
  "content_type": "news",
  "human_summary": "The claim about protests in Delhi is accurate."
}}

SPECIAL CASES:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. **Old News**: If claim is factual but refers to past events (>1 year old):
   - prediction: "Real" (if accurate at the time)
   - Add to explanation: "This refers to a past event from [date]"

2. **Satire/Parody**: If content appears satirical:
   - prediction: "Misleading"
   - explanation: "This appears to be satire or parody content | Not meant as factual reporting"

3. **Mixed Content**: Personal story with factual claims:
   - Focus only on verifiable facts
   - Ignore personal narrative elements

Return ONLY valid JSON. No additional text.
"""
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

def quick_initial_assessment(text: str) -> dict:
    """
    Fast lightweight Gemini-only initial impression.
    Returns a short neutral paragraph without predictions or scores.
    """
    prompt = f"""
    You are assisting in misinformation detection, but this is a *quick initial impression*.
    DO NOT claim anything is true or false and DO NOT assign confidence or numeric scores.

    Provide a short paragraph (3‚Äì5 sentences max) summarizing:
    - What type of content this text appears to be (news, opinion, speculation)
    - Whether it *sounds* factual or sensational
    - Whether anything seems unverifiable at first glance
    - Ask the user to wait for full fact-check and verification
    - Assume that the date is {datetime.now().strftime("%B %d, %Y")}

    Never assert factual accuracy.

    Text to evaluate:
    \"\"\"{text}\"\"\"
    """

    try:
        resp = GEM_MODEL.generate_content(prompt)
        return {
            "status": "ok",
            "initial_analysis": resp.text.strip(),
        }
    except Exception as e:
        return {
            "status": "error",
            "initial_analysis": "Could not analyze text.",
            "error": str(e)
        }

async def run_parallel_storage(text, score, label, explanation):
        loop = asyncio.get_running_loop()

        firestore = loop.run_in_executor(
            None,
            lambda text=text, score=score, label=label, explanation=explanation:
                store_in_firestore(text, score, label, explanation)
        )

        pinecone = loop.run_in_executor(
            None,
            lambda text=text, score=score, label=label, explanation=explanation:
                store_in_pinecone(text, score, label, explanation)
        )

        return await asyncio.gather(firestore, pinecone)

def detect_fake_text(text: str) -> dict:
    import asyncio, time, math, re

    start_total = time.time()
    text = re.sub(r"(?<=[a-zA-Z])\.(?=[A-Z])", ". ", text)

    # ----------------------------
    # PHASE 1 ‚Äì Fact check + metadata (parallel sync)
    # ----------------------------
    async def run_parallel_phase1():
        loop = asyncio.get_running_loop()
        fact_check = loop.run_in_executor(None, query_google_fact_check_api, text)
        metadata = loop.run_in_executor(None, extract_metadata_with_gemini, text)
        return await asyncio.gather(fact_check, metadata)

    # ----------------------------
    # PHASE 2 ‚Äì Vertex + Google corroboration (parallel async)
    # ----------------------------
    async def run_parallel_phase2(metadata):
        claims = simple_sentence_split(metadata["text"])
        combined_query = " OR ".join(claims[:3])

        loop = asyncio.get_running_loop()
        vertex_task = loop.run_in_executor(
            None, lambda: extract_vertex_scores(predict_with_vertex_ai(metadata))
        )
        corroboration_task = corroborate_all_with_google_async([combined_query])

        vertex_scores, corroboration_data = await asyncio.gather(vertex_task, corroboration_task)
        return vertex_scores, corroboration_data, claims

    # ----------------------------
    # PHASE 3 ‚Äì Per-claim evaluation (weighted ensemble)
    # ----------------------------
    async def run_parallel_claim_checks(claims, corroboration_data, fact_check_results, metadata, vertex_scores):

        async def process_claim(claim):
            loop = asyncio.get_running_loop()
            parsed = {}

            # If nothing corroborates & no fact checks found ‚Üí fallback "Unknown"
            if corroboration_data["status"] == "no_results" and fact_check_results["status"] == "no_fact_checks":
                gem_pred, gem_conf = "Unknown", 60
                evidence_strength = 0

                safe_vertex_scores = vertex_scores or {"Real": 0.7, "Fake": 0.2, "Misleading": 0.1}
                final_pred, final_conf = adjusted_ensemble(
                    gem_pred, gem_conf,
                    safe_vertex_scores,
                    fact_check_results.get("status", "no_fact_checks"),
                    corroboration_data.get("status", "no_results"),
                    evidence_strength
                )

            else:
                gem_resp = await loop.run_in_executor(
                    None,
                    lambda: ask_gemini_structured(
                        assemble_gemini_prompt_structured(
                            claim,
                            corroboration_data["evidences"],
                            corroboration_data["status"],
                            fact_check_results,
                            full_text=metadata["text"]
                        )
                    )
                )

                parsed = gem_resp.get("parsed", {}) or {}
                gem_pred = parsed.get("prediction", "Unknown")
                gem_conf = int(parsed.get("confidence", 70))

                # ‚úÖ Weighted evidence score (support - contradict)
                supports = sum((e.get("confidence", 50) / 100.0)
                               for e in corroboration_data.get("evidences", [])
                               if e.get("relevance") == "supports")

                contradicts = sum((e.get("confidence", 50) / 100.0)
                                  for e in corroboration_data.get("evidences", [])
                                  if e.get("relevance") == "contradicts")

                evidence_strength = round(supports - contradicts, 3)

                safe_vertex_scores = vertex_scores or {"Real": 0.7, "Fake": 0.2, "Misleading": 0.1}
                final_pred, final_conf = adjusted_ensemble(
                    gem_pred, gem_conf, safe_vertex_scores,
                    fact_check_results.get("status", "no_fact_checks"),
                    corroboration_data.get("status", "no_results"),
                    evidence_strength
                )

            explanation = parsed.get("explanation")
            if not explanation or "{" in explanation:
                explanation = f"{final_pred}: {final_conf}% | evidence={evidence_strength}"

            return {
                "claim_text": claim,
                "gemini": {"prediction": gem_pred, "confidence": gem_conf},
                "vertex_ai": vertex_scores,
                "fact_check": fact_check_results,
                "corroboration": corroboration_data,
                "ensemble": {"final_prediction": final_pred, "final_confidence": final_conf},
                "explanation": explanation,
                "evidence_strength": evidence_strength
            }

        return await asyncio.gather(*[process_claim(c) for c in claims])

    # ----------------------------
    # FULL PIPELINE EXECUTION
    # ----------------------------
    async def main():
        fact_check_results, metadata = await run_parallel_phase1()
        vertex_scores, corroboration_data, claims = await run_parallel_phase2(metadata)

        results = await run_parallel_claim_checks(
            claims, corroboration_data, fact_check_results, metadata, vertex_scores
        )

        label_stats = {}
        for r in results:
            lbl = r["ensemble"]["final_prediction"]
            conf = r["ensemble"]["final_confidence"]

            if lbl not in label_stats:
                label_stats[lbl] = {"sum_conf": 0.0, "count": 0}

            label_stats[lbl]["sum_conf"] += conf
            label_stats[lbl]["count"] += 1

        # --------------------------
        # Step 1: majority vote
        # --------------------------
        majority_label = max(label_stats.items(), key=lambda x: x[1]["count"])[0]
        majority_count = label_stats[majority_label]["count"]

        # If majority label appears more than 50%, use it directly
        if majority_count > len(results) / 2:
            overall_label = majority_label
        else:
            # otherwise: tie or fragmented ‚Üí use highest avg confidence ONLY as tie breaker
            overall_label = max(label_stats.items(), key=lambda x: x[1]["sum_conf"] / x[1]["count"])[0]

        # final confidence = average confidence of selected label
        chosen = label_stats[overall_label]
        avg_conf = chosen["sum_conf"] / chosen["count"]

        # Reduce confidence inflation from single outliers
        overall_conf = int(min(100, avg_conf * (0.65 + 0.35 * math.log1p(chosen["count"]))))


        combined_explanation = " | ".join(r["explanation"] for r in results[:3])

        await run_parallel_storage(text, overall_conf, overall_label, combined_explanation)

        return {
            "summary": {
                "score": overall_conf,
                "prediction": overall_label,
                "explanation": combined_explanation,
            },
            "runtime": round(time.time() - start_total, 2),
            "claims_checked": len(results),
            "raw_details": results
        }

    return asyncio.run(main())